# -*- coding: utf-8 -*-
"""smoke_detection_tf.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/github/DivyaAmirtharaj/cs249_final_project/blob/main/smoke_detection_tf.ipynb
"""

# 1. Import
## Prepare Data

from roboflow import Roboflow
rf = Roboflow(api_key="kurHDAP4tAun3nyyYi4d")
project = rf.workspace("wildfire-xpwrf").project("wildfire-4tdl8")

# Specify the desired version of the dataset and the output directory
output_directory = "/content/Wildfire"
dataset = project.version(2).download("tfrecord", output_directory)

"""
# 2. Train
## Configs and Hyperparameters
"""

repo_url = 'https://github.com/roboflow-ai/tensorflow-object-detection-faster-rcnn'

# Number of training steps - 1000 will train very quickly, but more steps will increase accuracy.
num_steps = 1000  # 200000 to improve

# Number of evaluation steps.
num_eval_steps = 50

MODELS_CONFIG = {
    'ssd_mobilenet_v2': {
        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',
        'pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',
        'batch_size': 12
    },
    'faster_rcnn': { # Update to make compatible with TF2
        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu',
        'pipeline_file': 'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.config',
        'batch_size': 12
    },
    'efficientdet': { # Update to make compatible with TF2
        'model_name': 'ssd_efficientdet_d4_1024x1024_coco17_tpu-32',
        'pipeline_file': 'ssd_efficientdet_d4_1024x1024_coco17_tpu-32.config',
        'batch_size': 8
    },
}


# Pick the model you want to use
# Select a model in `MODELS_CONFIG`.
selected_model = 'ssd_mobilenet_v2'

# Name of the object detection model to use.
MODEL = MODELS_CONFIG[selected_model]['model_name']

# Name of the pipline file in tensorflow object detection API.
pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']

# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.
batch_size = MODELS_CONFIG[selected_model]['batch_size']

"""## Clone the `tensorflow-object-detection` repository or your fork."""

# Commented out IPython magic to ensure Python compatibility.
import os

# %cd /content

repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))

!git clone {repo_url}
# %cd {repo_dir_path}
!git pull

"""## Install required packages"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!git clone --quiet https://github.com/tensorflow/models.git

!pip install tf_slim

!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk

!pip install -q Cython contextlib2 pillow lxml matplotlib

!pip install -q pycocotools

!pip install tensorflow_io

!pip install tf-models-official

# TF 2.13.0 is the only compatible version
!pip uninstall -y tensorflow
!pip install tensorflow==2.13.0

# %cd /content/models/research
!protoc object_detection/protos/*.proto --python_out=.

import os
os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'

!python object_detection/builders/model_builder_test.py

# Verify we are using Tensorflow 2.13.0
import tensorflow as tf

print("TensorFlow version:", tf.__version__)

"""## Prepare `tfrecord` files

Roboflow automatically creates our TFRecord and label_map files that we need!

**Generating your own TFRecords the only step you need to change for your own custom dataset.**

Because we need one TFRecord file for our training data, and one TFRecord file for our test data, we'll create two separate datasets in Roboflow and generate one set of TFRecords for each.

To create a dataset in Roboflow and generate TFRecords, follow [this step-by-step guide](https://blog.roboflow.ai/getting-started-with-roboflow/).
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/tensorflow-object-detection-faster-rcnn/data

!cp -r /content/Wildfire/train/ /content/tensorflow-object-detection-faster-rcnn/data/
!cp -r /content/Wildfire/test/ /content/tensorflow-object-detection-faster-rcnn/data/

# Commented out IPython magic to ensure Python compatibility.
# training set
# %ls train

# Commented out IPython magic to ensure Python compatibility.
# test set
# %ls test

test_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/test/fire.tfrecord'
train_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/train/fire.tfrecord'
label_map_pbtxt_fname = '/content/tensorflow-object-detection-faster-rcnn/data/train/fire_label_map.pbtxt'

"""## Download base model"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/models/research

import os
import shutil
import glob
import urllib.request
import tarfile
MODEL_FILE = MODEL + '.tar.gz'
print(MODEL_FILE)
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'
print(DOWNLOAD_BASE)
DEST_DIR = '/content/models/research/pretrained_model'

if not (os.path.exists(MODEL_FILE)):
    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)

tar = tarfile.open(MODEL_FILE)
tar.extractall()
tar.close()

os.remove(MODEL_FILE)
if (os.path.exists(DEST_DIR)):
    shutil.rmtree(DEST_DIR)
os.rename(MODEL, DEST_DIR)

!echo {DEST_DIR}
!ls -alh {DEST_DIR}

import os

directory_path = '/content/models/research/pretrained_model/checkpoint'
os.listdir(directory_path)

# Fix checkpoint naming scheme
import os
import glob

directory_path = '/content/models/research/pretrained_model/checkpoint'

# Loop through all files that start with 'model.ckpt-1.'
for filepath in glob.glob(os.path.join(directory_path, 'ckpt-0.*')):
    base_directory, filename = os.path.split(filepath)
    file_extension = filename.split('.', 2)[-1]

    # Construct the new filename
    new_filename = 'model.ckpt.' + file_extension

    # Construct the full path for the new file
    new_filepath = os.path.join(base_directory, new_filename)

    # Rename the file
    os.rename(filepath, new_filepath)
    print(f'Renamed {filepath} to {new_filepath}')

# Verify naming scheme of checkpoints
import os

directory_path = '/content/models/research/pretrained_model/checkpoint'
os.listdir(directory_path)

"""## Configuring a Training Pipeline"""

import os
pipeline_fname = os.path.join('/content/models/research/object_detection/configs/tf2/', pipeline_file)
print(pipeline_file)

assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)

def get_num_classes(pbtxt_fname):
    from object_detection.utils import label_map_util
    label_map = label_map_util.load_labelmap(pbtxt_fname)
    categories = label_map_util.convert_label_map_to_categories(
        label_map, max_num_classes=90, use_display_name=True)
    category_index = label_map_util.create_category_index(categories)
    return len(category_index.keys())

import re

train_record = "/content/tensorflow-object-detection-faster-rcnn/data/train/fire.tfrecord"
train_label_map = "/content/tensorflow-object-detection-faster-rcnn/data/train/fire_label_map.pbtxt"

test_record = "/content/tensorflow-object-detection-faster-rcnn/data/test/fire.tfrecord"
test_label_map = "/content/tensorflow-object-detection-faster-rcnn/data/test/fire_label_map.pbtxt"

checkpoint = "/content/models/research/pretrained_model/checkpoint/model.ckpt"

num_classes = get_num_classes(label_map_pbtxt_fname)

with open(pipeline_fname) as f:
    s = f.read()
with open(pipeline_fname, 'w') as f:

    # fine_tune_checkpoint
    s = re.sub('fine_tune_checkpoint: ".*?"',
               'fine_tune_checkpoint: "{}"'.format(checkpoint), s)

    s = re.sub('fine_tune_checkpoint_type: ".*?"',
               'fine_tune_checkpoint_type: "detection"', s)

    # label_map_path
    s = re.sub(r'(train_input_reader: {[\s\S]*?label_map_path: ")[^"]*(")',
               r'\1{}\2'.format(train_label_map), s)
    s = re.sub(r'(eval_input_reader: {[\s\S]*?label_map_path: ")[^"]*(")',
               r'\1{}\2'.format(test_label_map), s)

    # tfrecord paths
    s = re.sub(
        r'(train_input_reader: {\s+label_map_path: ".*?"\s+tf_record_input_reader {\s+input_path: ").*?(")',
        r'\1{}\2'.format(train_record), s)
    s = re.sub(
        r'(eval_input_reader: \{\s+label_map_path: ".*?"\s+shuffle: \w+\s+num_epochs: \d+\s+tf_record_input_reader \{\s+input_path: ").*?(")',
        r'\1{}\2'.format(test_record), s, flags=re.DOTALL)


    # Set training batch_size.
    s = re.sub('batch_size: [0-9]+',
               'batch_size: {}'.format(batch_size), s)

    # Set training steps, num_steps
    s = re.sub('num_steps: [0-9]+',
               'num_steps: {}'.format(num_steps), s)

    # Set number of classes num_classes.
    s = re.sub('num_classes: [0-9]+',
               'num_classes: {}'.format(num_classes), s)
    f.write(s)

!cat {pipeline_fname}

model_dir = '../../../content/training/'
# Optionally remove content in output model directory to fresh start.
!rm -rf {model_dir}
os.makedirs(model_dir, exist_ok=True)

"""## Run Tensorboard(Optional)"""

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip -o ngrok-stable-linux-amd64.zip

LOG_DIR = model_dir
get_ipython().system_raw(
    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'
    .format(LOG_DIR)
)

get_ipython().system_raw('./ngrok http 6006 &')

"""### Get Tensorboard link"""

! curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

"""## Train the model"""

!pip install lvis
!python /content/models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path={pipeline_fname} \
    --model_dir={model_dir} \
    --alsologtostderr \
    --num_train_steps={num_steps} \
    --num_eval_steps={num_eval_steps}

!ls {model_dir}

"""## Exporting Trained Inference Graphs
Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. We do this again for TFLite. This can be done as follows:
"""

import os
import tensorflow as tf
from object_detection.utils import config_util
from object_detection.builders import model_builder

# Set paths
output_directory = './fine_tuned_model'
tflite_directory = './fine_tuned_model/tflite'

lst = os.listdir(model_dir)
lst = [l for l in lst if 'ckpt-' in l and '.data' in l]

steps=np.array([int(re.findall('\d+', l)[0]) for l in lst])
last_model = lst[steps.argmax()].split('.data')[0]
print(last_model)

last_model_path = os.path.join(model_dir, last_model)
print(last_model_path)


# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(pipeline_fname)
model_config = configs['model']
detection_model = model_builder.build(model_config=model_config, is_training=False)

# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(model_dir, last_model)).expect_partial()  # Update 'ckpt-XX' to your latest checkpoint

# Function to run inference on a dummy image to build the model
@tf.function
def detect_fn(image):
    image, shapes = detection_model.preprocess(image)
    prediction_dict = detection_model.predict(image, shapes)
    detections = detection_model.postprocess(prediction_dict, shapes)
    return detections

# Save the model in SavedModel format
tf.saved_model.save(detection_model, output_directory, signatures=detect_fn.get_concrete_function(tf.TensorSpec(shape=[1, None, None, 3], dtype=tf.float32)))

# Convert to TensorFlow Lite format
converter = tf.lite.TFLiteConverter.from_saved_model(output_directory)
tflite_model = converter.convert()

# Save the TFLite model
with open(os.path.join(tflite_directory, 'model.tflite'), 'wb') as f:
    f.write(tflite_model)

# Verify model was built
import os

print(os.listdir(output_directory))
print(os.listdir(tflite_directory))

full_paths_output_dir = [os.path.join(os.path.abspath(output_directory), f) for f in os.listdir(output_directory)]
print(full_paths_output_dir)

full_paths_tflite_dir = [os.path.join(os.path.abspath(tflite_directory), f) for f in os.listdir(tflite_directory)]
print(full_paths_tflite_dir)

"""## Download the model `.pb` file"""

import os

saved_model_pb_fname = os.path.join(os.path.abspath(output_directory), "saved_model.pb")

# Check if the SavedModel file exists
print(saved_model_pb_fname)
assert os.path.isfile(saved_model_pb_fname), '`{}` does not exist.'.format(saved_model_pb_fname)

!ls -alh {saved_model_pb_fname}

"""### Upload the `.pb` file to your Google Drive
Then download it from your Google Drive to local file system.

During this step, you will be prompted to enter the token.
"""

# # Install the PyDrive wrapper & import libraries.
# # This only needs to be done once in a notebook.
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials


# # Authenticate and create the PyDrive client.
# # This only needs to be done once in a notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

fname = os.path.basename(saved_model_pb_fname)
# # Create & upload a text file.
uploaded = drive.CreateFile({'title': fname})
uploaded.SetContentFile(saved_model_pb_fname)
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))

"""### Download the `.pb` file directly to your local file system
This method may not be stable when downloading large files like the model `.pb` file. Try **option 1** instead if not working.
"""

from google.colab import files
files.download(saved_model_pb_fname)

"""### Download CONTENT files as a zip and upload to Github

We have created many changes in the config files as well as in datasets, checkpoint generation, and model generation, please upload these changes to Github: [git@github.com:DivyaAmirtharaj/cs249_final_project.git](https://)
"""

# Zip all files and download
!zip -r /content/smoke_detection_tfmodel.zip /content

from google.colab import files
files.download("/content/smoke_detection_tfmodel.zip")

"""### OPTIONAL: Download the `label_map.pbtxt` file"""

from google.colab import files
files.download(label_map_pbtxt_fname)

"""### OPTIONAL: Download the modified pipline file
If you plan to use OpenVINO toolkit to convert the `.pb` file to inference faster on Intel's hardware (CPU/GPU, Movidius, etc.)
"""

files.download(pipeline_fname)

!tar cfz fine_tuned_model.tar.gz fine_tuned_model
from google.colab import files
files.download('fine_tuned_model.tar.gz')

"""## Run inference test
Test with images in repository `tensorflow-object-detection/test` directory.

**To test with your own images, you need to place your images inside the `test` directory in this Colab notebook!** More on this below.
"""



from google.colab import drive
drive.mount('/content/drive/', force_remount=True)
import os

!ls /content/drive/MyDrive/cs249_final_project/generated_model_12.5.2023.1435/cs249_final_content
!cp -r /content/drive/MyDrive/cs249_final_project/generated_model_12.5.2023.1435/cs249_final_content ../../downloaded_content

!ls ../../downloaded_content/content/tensorflow-object-detection-faster-rcnn/data/test
!cp -r ../../downloaded_content/content/tensorflow-object-detection-faster-rcnn/data/test ./test

!ls ../../downloaded_content/content/models/research/fine_tuned_model
!cp -r ../../downloaded_content/content/models/research/fine_tuned_model ./fine_tuned_model

!ls fine_tuned_model/

!cp -r /content/test/test/ /content/tensorflow-object-detection-faster-rcnn/

import os
import glob

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_CKPT = "/content/downloaded_content/content/models/research/fine_tuned_model/"

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = "/content/downloaded_content/content/tensorflow-object-detection-faster-rcnn/data/test/fire_label_map.pbtxt"
print(PATH_TO_LABELS)

# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.
# PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, "test")

# assert os.path.isfile(saved_model_pb_fname)
# assert os.path.isfile(PATH_TO_LABELS)
# TEST_IMAGE_PATHS = glob.glob(os.path.join(TEST_IMAGES_PATH, "*.*"))
# assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)
# print(TEST_IMAGE_PATHS)

# Path to the TFRecord file
PATH_TO_TFRECORD = "/content/downloaded_content/content/tensorflow-object-detection-faster-rcnn/data/test/fire.tfrecord"
assert os.path.isfile(PATH_TO_TFRECORD), 'TFRecord file not found at `{}`.'.format(PATH_TO_TFRECORD)

# Load and parse the TFRecord
raw_dataset = tf.data.TFRecordDataset(PATH_TO_TFRECORD)
image_feature_description = {
    'image/encoded': tf.io.FixedLenFeature([], tf.string),
    # Add additional features if needed (like labels, format, etc.)
}

def _parse_image_function(example_proto):
    # Parse the input tf.Example proto using the dictionary above.
    return tf.io.parse_single_example(example_proto, image_feature_description)

parsed_image_dataset = raw_dataset.map(_parse_image_function)

# Commented out IPython magic to ensure Python compatibility.
from PIL import Image
import io
import matplotlib.pyplot as plt
# %matplotlib inline

# ... [previous code for loading and parsing the TFRecord]

# Function to decode and display an image
def display_image(image_raw):
    image = Image.open(io.BytesIO(image_raw))
    plt.imshow(image)
    plt.axis('off')  # Turn off axis numbers
    plt.show()

# Iterate over the dataset and display images
for image_features in parsed_image_dataset.take(2):  # Adjust the number as needed
    image_raw = image_features['image/encoded'].numpy()
    display_image(image_raw)

!ls /content/downloaded_content/content/models/research/object_detection

!ls /content/

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/downloaded_content/content/models/research/object_detection


import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops


# This is needed to display the images.
# %matplotlib inline


from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util

import tensorflow as tf
import numpy as np
from object_detection.utils import label_map_util
from object_detection.utils import visualization_utils as vis_util

# Load the saved model
detection_model = tf.saved_model.load(PATH_TO_CKPT)

# Load the label map
num_classes = get_num_classes(PATH_TO_LABELS)
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(
    label_map, max_num_classes=num_classes, use_display_name=True)
category_index = label_map_util.create_category_index(categories)


def load_image_into_numpy_array(image):
    (im_width, im_height) = image.size
    return np.array(image.getdata()).reshape(
        (im_height, im_width, 3)).astype(np.uint8)

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)

def run_inference_for_single_image(model, image):
    image = np.asarray(image)
    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.
    input_tensor = tf.convert_to_tensor(image)
    # The model expects a batch of images, so add an axis with `tf.newaxis`.
    input_tensor = input_tensor[tf.newaxis,...]

    # Run inference
    model_fn = model.signatures['serving_default']
    output_dict = model_fn(input_tensor)

    # All outputs are batches tensors.
    # Convert to numpy arrays, and take index [0] to remove the batch dimension.
    # We're only interested in the first num_detections.
    num_detections = int(output_dict.pop('num_detections'))
    output_dict = {key: value[0, :num_detections].numpy()
                   for key, value in output_dict.items()}
    output_dict['num_detections'] = num_detections

    # detection_classes should be ints.
    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)

    # Handle models with masks:
    if 'detection_masks' in output_dict:
        # Reframe the the bbox mask to the image size.
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
                    output_dict['detection_masks'], output_dict['detection_boxes'],
                     image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,
                                           tf.uint8)
        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()

    return output_dict

# Commented out IPython magic to ensure Python compatibility.
# Output images not showing? Run this cell again, and try the cell above
# This is needed to display the images.
# %matplotlib inline

# Limited displayed images to 5 to reduce memory

def load_and_preprocess_image(parsed_data):
    image = tf.image.decode_image(parsed_data['image/encoded'], channels=3)
    image = tf.cast(image, tf.float32) / 255.0  # Normalize and convert to float32

    # Resize the image to match the expected input size of the model
    # Replace (640, 640) with the actual expected size if different
    image = tf.image.resize(image, (640, 640))
    return image.numpy()


count = 5
for i, image_features in enumerate(parsed_image_dataset.take(count)):
    # Convert the parsed data into a numpy array
    image_np = load_and_preprocess_image(image_features)

    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
    # image_np_expanded = np.expand_dims(image_np, axis=0)
    # print(image_np_expanded.shape)

    # Actual detection
    output_dict = run_inference_for_single_image(detection_model, image_np)

    # Visualization of the results of a detection
    print(output_dict)
    vis_util.visualize_boxes_and_labels_on_image_array(
        image_np,
        output_dict['detection_boxes'],
        output_dict['detection_classes'],
        output_dict['detection_scores'],
        category_index,
        instance_masks=output_dict.get('detection_masks_reframed', None),
        use_normalized_coordinates=True,
        line_thickness=8)

    # Display the image
    plt.figure(figsize=IMAGE_SIZE)
    plt.imshow(image_np)
    plt.show()

"""# 3. Convert

With our model trained, it's time to convert our graph to .tflite!
"""

# Commented out IPython magic to ensure Python compatibility.
# %ls /content/models/research/fine_tuned_model/tflite

!tflite_convert \
  --input_shape=1,224,224,3 \
  --input_arrays=normalized_input_image_tensor \
  --output_arrays=MobilenetV1/Predictions/Softmax \
  --allow_custom_ops \
  --graph_def_file=/content/models/research/fine_tuned_model/tflite/tflite_graph.pb \
  --output_file="/content/models/research/fine_tuned_model/final_model.tflite"

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import tensorflow as tf

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter('/content/drive/MyDrive/final_model.tflite')
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test model on random input data.
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)

# Change the final TFLite destination here
!cp /content/models/research/fine_tuned_model/final_model.tflite "/content/drive/My Drive/"


"""Your TFLite file is now in your Drive as "final_model.tflite", ready to use with your project on-device! For specific device tutorials, check out the official TensorFlow Lite [Android Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), [iOS Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/ios), or [Raspberry Pi Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi). [link text](https://)"""
